stopifnot(isTRUE(all.equal(rho(2, 2), 4)))
stopifnot(isTRUE(all.equal(rho(2, 1), 3)))
# Basic Tukey function
Tukey = function(beta, x, y, k=19000) {
return (1 / length(x) * sum(rho(y - beta[1] - beta[2] * x, k)))
}
Tukey.gradient = function(beta, x, y, k=19000) {
c(-1/length(x) * sum(rho.prime(y - beta[1] - beta[2] * x, k)),
-1/length(x) * sum(x * rho.prime(y - beta[1] - beta[2] * x, k)))
}
# a) Make a scatterplot of farm vs land.
plot <- area %>% ggplot(aes(x = land, y = farm)) +
geom_point(shape=1, size=1.5) +
labs(x = "total land area (sq. miles)", y = "farm land area (sq. miles)")
# Create the least squares regression model
lm.model = lm(farm ~ land, data=area)
# Add the lm abline to the plot
plot = plot + geom_abline(aes(intercept = lm.model$coefficients[1], slope = lm.model$coefficients[2]), colour='#32CD32')
# Use the Nelder-Mead method in optim() to estimate
nm.model <- optim(par=c(0,0), fn=Tukey, method="Nelder-Mead", x=area$land, y=area$farm)
#print(nm.model)
plot = plot + geom_abline(aes(intercept = nm.model$par[1],slope = nm.model$par[2]),colour = 'blue')
# BFGS
bfgs.model <- optim(par=c(0,0), fn=Tukey, gr = Tukey.gradient, method="BFGS", x=area$land, y=area$farm)
#print(bfgs.model)
plot = plot + geom_abline(aes(intercept = bfgs.model$par[1], slope = bfgs.model$par[2]), colour="black")
# CG
cg.model <- optim(par=c(0,0), fn=Tukey, gr=Tukey.gradient, method="CG", x=area$land, y=area$farm)
print(cg.model)
plot = plot + geom_abline(aes(intercept = cg.model$par[1], slope = cg.model$par[2]), colour="coral", lty="dashed")
# TODO - Add a legend to the plot - not working, come back
plot = plot + scale_fill_discrete(labels = c("lm()", "NM", "BFGS", "CG")) + theme(legend.position="topright")
# Print the plot
print(plot)
print(nm.model$value)
print(bfgs.model$value)
print(cg.model$value)
t.squared = function(x) {
return(x^2)
}
plot2 = curve(expr = rho, from=-100000, to=100000, n=200000, ylim=c(0, 9e+09))
plot3 = curve(expr = t.squared, from=-100000, to=100000, n=200000, add=TRUE, col="red", ylim=c(0, 9e+09))
legend("top", legend=c(expression(rho(t)), expression(t^2)), col=c("black", "red"), lty=c(1,1))
require(datasets)
str(nhtemp)
y.hat.function = function(beta, y, y.hat) {
return(beta * y + (1 - beta) * y.hat)
}
forecast.error = function(beta, y) {
total = 0
y.hat.last = y[1]
for (i in iter) {
y.hat = y.hat.function(beta[1], y[i-1], y.hat.last)
total = total + (y[i] - y.hat)^2
y.hat.last = y.hat
}
return(1 / length(y) * total)
}
library(ggplot2)
library(tidyverse)
library(dplyr)
area <- read.csv("farmLandArea.csv")
str(area)
# Basic rho function
rho = function(t, k=19000) {
ifelse(abs(t) <= k, t^2, 2 * k * abs(t) - k^2)
}
rho.prime = function(t, k=19000) {
ifelse(abs(t) <= k, 2 * t, 2 * k * sign(t))
}
# Test cases for rho
stopifnot(isTRUE(all.equal(rho(0, 2), 0)))
stopifnot(isTRUE(all.equal(rho(1, 2), 1)))
stopifnot(isTRUE(all.equal(rho(2, 2), 4)))
stopifnot(isTRUE(all.equal(rho(2, 1), 3)))
# Basic Tukey function
Tukey = function(beta, x, y, k=19000) {
return (1 / length(x) * sum(rho(y - beta[1] - beta[2] * x, k)))
}
Tukey.gradient = function(beta, x, y, k=19000) {
c(-1/length(x) * sum(rho.prime(y - beta[1] - beta[2] * x, k)),
-1/length(x) * sum(x * rho.prime(y - beta[1] - beta[2] * x, k)))
}
# a) Make a scatterplot of farm vs land.
plot <- area %>% ggplot(aes(x = land, y = farm)) +
geom_point(shape=1, size=1.5) +
labs(x = "total land area (sq. miles)", y = "farm land area (sq. miles)")
# Create the least squares regression model
lm.model = lm(farm ~ land, data=area)
# Add the lm abline to the plot
plot = plot + geom_abline(aes(intercept = lm.model$coefficients[1], slope = lm.model$coefficients[2]), colour='#32CD32')
# Use the Nelder-Mead method in optim() to estimate
nm.model <- optim(par=c(0,0), fn=Tukey, method="Nelder-Mead", x=area$land, y=area$farm)
#print(nm.model)
plot = plot + geom_abline(aes(intercept = nm.model$par[1],slope = nm.model$par[2]),colour = 'blue')
# BFGS
bfgs.model <- optim(par=c(0,0), fn=Tukey, gr = Tukey.gradient, method="BFGS", x=area$land, y=area$farm)
#print(bfgs.model)
plot = plot + geom_abline(aes(intercept = bfgs.model$par[1], slope = bfgs.model$par[2]), colour="black")
# CG
cg.model <- optim(par=c(0,0), fn=Tukey, gr=Tukey.gradient, method="CG", x=area$land, y=area$farm)
print(cg.model)
plot = plot + geom_abline(aes(intercept = cg.model$par[1], slope = cg.model$par[2]), colour="coral", lty="dashed")
# TODO - Add a legend to the plot - not working, come back
plot = plot + scale_fill_discrete(labels = c("lm()", "NM", "BFGS", "CG")) + theme(legend.position="topright")
# Print the plot
print(plot)
print(nm.model$value)
print(bfgs.model$value)
print(cg.model$value)
t.squared = function(x) {
return(x^2)
}
plot2 = curve(expr = rho, from=-100000, to=100000, n=200000, ylim=c(0, 9e+09))
plot3 = curve(expr = t.squared, from=-100000, to=100000, n=200000, add=TRUE, col="red", ylim=c(0, 9e+09))
legend("top", legend=c(expression(rho(t)), expression(t^2)), col=c("black", "red"), lty=c(1,1))
require(datasets)
str(nhtemp)
y.hat.function = function(beta, y, y.hat) {
return(beta * y + (1 - beta) * y.hat)
}
forecast.error = function(beta, y) {
total = 0
y.hat.last = y[1]
for (i in iter) {
y.hat = y.hat.function(beta[1], y[i-1], y.hat.last)
total = total + (y[i] - y.hat)^2
y.hat.last = y.hat
}
return(1 / length(y) * total)
}
# Make a copy
dat = nhtemp
#str(dat)
iter = seq(2, length(y))
# Make a copy
dat = nhtemp
#str(dat)
iter = seq(2, length(dat))
# Use optimize to find beta
b.dat = optimize(f = forecast.error, interval = c(0, 1), y = dat)
b.dat
beta = b.dat$minimum
# Beta equals the minimum:
cat(sep="", "Beta = ", beta,"\n")
HoltWinters(nhtemp, beta=FALSE, gamma=FALSE)
library(ggplot2)
library(tidyverse)
library(dplyr)
area <- read.csv("farmLandArea.csv")
str(area)
# Basic rho function
rho = function(t, k=19000) {
ifelse(abs(t) <= k, t^2, 2 * k * abs(t) - k^2)
}
rho.prime = function(t, k=19000) {
ifelse(abs(t) <= k, 2 * t, 2 * k * sign(t))
}
# Test cases for rho
stopifnot(isTRUE(all.equal(rho(0, 2), 0)))
stopifnot(isTRUE(all.equal(rho(1, 2), 1)))
stopifnot(isTRUE(all.equal(rho(2, 2), 4)))
stopifnot(isTRUE(all.equal(rho(2, 1), 3)))
# Basic Tukey function
Tukey = function(beta, x, y, k=19000) {
return (1 / length(x) * sum(rho(y - beta[1] - beta[2] * x, k)))
}
Tukey.gradient = function(beta, x, y, k=19000) {
c(-1/length(x) * sum(rho.prime(y - beta[1] - beta[2] * x, k)),
-1/length(x) * sum(x * rho.prime(y - beta[1] - beta[2] * x, k)))
}
# a) Make a scatterplot of farm vs land.
plot <- area %>% ggplot(aes(x = land, y = farm)) +
geom_point(shape=1, size=1.5) +
labs(x = "total land area (sq. miles)", y = "farm land area (sq. miles)")
# Create the least squares regression model
lm.model = lm(farm ~ land, data=area)
# Add the lm abline to the plot
plot = plot + geom_abline(aes(intercept = lm.model$coefficients[1], slope = lm.model$coefficients[2]), colour='#32CD32')
# Use the Nelder-Mead method in optim() to estimate
nm.model <- optim(par=c(0,0), fn=Tukey, method="Nelder-Mead", x=area$land, y=area$farm)
#print(nm.model)
plot = plot + geom_abline(aes(intercept = nm.model$par[1],slope = nm.model$par[2]),colour = 'blue')
# BFGS
bfgs.model <- optim(par=c(0,0), fn=Tukey, gr = Tukey.gradient, method="BFGS", x=area$land, y=area$farm)
#print(bfgs.model)
plot = plot + geom_abline(aes(intercept = bfgs.model$par[1], slope = bfgs.model$par[2]), colour="black")
# CG
cg.model <- optim(par=c(0,0), fn=Tukey, gr=Tukey.gradient, method="CG", x=area$land, y=area$farm)
print(cg.model)
plot = plot + geom_abline(aes(intercept = cg.model$par[1], slope = cg.model$par[2]), colour="coral", lty="dashed")
# TODO - Add a legend to the plot - not working, come back
plot = plot + scale_fill_discrete(labels = c("lm()", "NM", "BFGS", "CG")) + theme(legend.position="topright")
# Print the plot
print(plot)
print(nm.model$value)
print(bfgs.model$value)
print(cg.model$value)
t.squared = function(x) {
return(x^2)
}
plot2 = curve(expr = rho, from=-100000, to=100000, n=200000, ylim=c(0, 9e+09))
plot3 = curve(expr = t.squared, from=-100000, to=100000, n=200000, add=TRUE, col="red", ylim=c(0, 9e+09))
legend("top", legend=c(expression(rho(t)), expression(t^2)), col=c("black", "red"), lty=c(1,1))
require(datasets)
str(nhtemp)
y.hat.function = function(beta, y, y.hat) {
return(beta * y + (1 - beta) * y.hat)
}
forecast.error = function(beta, y) {
total = 0
y.hat.last = y[1]
for (i in iter) {
y.hat = y.hat.function(beta[1], y[i-1], y.hat.last)
total = total + (y[i] - y.hat)^2
y.hat.last = y.hat
}
return(1 / length(y) * total)
}
# Make a copy
dat = nhtemp
#str(dat)
iter = seq(2, length(dat))
# Use optimize to find beta
b.dat = optimize(f = forecast.error, interval = c(0, 1), y = dat)
b.dat
beta = b.dat$minimum
# Beta equals the minimum:
cat(sep="", "Beta = ", beta,"\n")
HoltWinters(nhtemp, beta=FALSE, gamma=FALSE)
# Plot the original data
plot(nhtemp)
dat = nhtemp
smooth.dat = nhtemp
head(smooth.dat)
beta
for (i in iter) {
smooth.dat[i] = beta * dat[i-1] + (1 - beta) * smooth.dat[i-1]
print(smooth.dat[i])
}
lines(smooth.dat, col="red")
getSmoothData = function(beta, dat) {
smooth.dat = dat
for (i in iter) {
smooth.dat[i] = beta * dat[i-1] + (1 - beta) * smooth.dat[i-1]
}
return(smooth.dat)
}
plot(nhtemp)
lines(getSmoothData(0.186, nhtemp), col="red")
lines(getSmoothData(0.1, nhtemp), col="blue")
lines(getSmoothData(0.9, nhtemp), col="green")
legend("topleft", legend=c("data", "smooth, beta=0.186", "smooth, beta=0.1", "smooth, beta=0.9"), col=c("black", "red", "blue", "green"), lty=c(1,1,1,1))
# ...
require("MASS")
menarche[c(1, 10, 25), ]
success.indices = rep(x=seq_len(nrow(menarche)), times=menarche$Menarche)
success.ages = menarche$Age[success.indices]
success = data.frame(age=success.ages, reached.menarche=1)
failure.indices = rep(x=seq_len(nrow(menarche)), times=menarche$Total - menarche$Menarche)
failure.ages = menarche$Age[failure.indices]
failure = data.frame(age=failure.ages, reached.menarche=0)
menarche.cases = rbind(success, failure)
menarche.cases = menarche.cases[order(menarche.cases$age), ]
rownames(menarche.cases) = NULL # Remove incorrectly ordered rownames; they get restored correctly.
menarche.cases[c(1000, 1500, 2000), ]
# ...
setwd("~/Google Drive/Classes/DS 202 - Exploratory Data Analysis/Labs/Lab5")
knitr::opts_chunk$set(echo = TRUE)
acc <- read.csv("https://raw.githubusercontent.com/xdaiISU/ds202materials/master/hwlabs/fars2017/accident.csv")
acc <- read.csv("https://raw.githubusercontent.com/xdaiISU/ds202materials/master/hwlabs/fars2017/accident.csv")
names(acc)
per <- read.csv("https://raw.githubusercontent.com/xdaiISU/ds202materials/master/hwlabs/fars2017/person.csv")
acc <- read.csv("accident.csv", stringAsFactors=FALSE)
setwd("~/Google Drive/Classes/DS 202 - Exploratory Data Analysis/Labs/Lab5")
acc <- read.csv("accident.csv", stringsAsFactors=FALSE)
names(acc)
per <- read.csv("person.csv", stringsAsFactors=FALSE)
names(per)
library(lubridate)
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(dplyr)
library(ggplot2)
names(acc)
dow <- acc$DAY_WEEK
str(dow)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram()
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(binwidth=5)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(binwidth=7)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(binwidth=10)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram()
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(binwidth=20)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(binwidth=30)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(bins=30)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(bins=7)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(bins=14)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(bins=12)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(bins=10)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(bins=7)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram()
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(bins=7)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(bins=14)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_histogram(bins=7)
?is.na
?read.csv
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_col()
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_bar()
# First find out how many NAs are in the data
x <- is.na(acc$DAY_WEEK)
# First find out how many NAs are in the data
(x <- is.na(acc$DAY_WEEK))
# First find out how many NAs are in the data
(x <- isTRUE(is.na(acc$DAY_WEEK)))
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip()
str(acc$HOUR)
head(acc$HOUR, 100)
acc %>% ggplot(aes(y = HOUR)) + geom_bar() + coord_flip()
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip()
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(ylim=c(0, 24))
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(xlim=c(0, 24))
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23))
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23)) + scale_fill_manual(limit=seq(0,23))
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23)) + scale_fill_manual(limits=seq(0,23))
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23)) + scale_fill_manual(breaks=seq(0,23))
head(acc$DRUNK_DR)
head(acc$DRUNK_DR, 100)
str(acc$DRUNK_DR)
x = c(0, 1 , 1, 1)
y = x > 0
y
length(x > 0)
length(x)
x
rm(list=ls())
# Load the CSV files
acc <- read.csv("accident.csv", stringsAsFactors=FALSE)
names(acc)
per <- read.csv("person.csv", stringsAsFactors=FALSE)
names(per)
x = c(0, 1, 1, 1)
x = acc$DRUNK_DR > 1
sum(x[TRUE])
sum(acc$DRUNK_DR > 1)
sum(x[TRUE])
sum(acc$DRUNK_DR == 0)
sum(acc$DRUNK_DR != 0)
str(per)
per_type = per$PER_TYP == 1
str(per_type)
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(dplyr)
library(ggplot2)
rm(list=ls())
# Load the accident CSV file
acc <- read.csv("accident.csv", stringsAsFactors=FALSE)
# names(acc)
acc %>% ggplot(aes(x = DAY_WEEK)) + geom_bar()
# Come back and clean this presentation up
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23)) + scale_fill_manual(breaks=seq(0,23))
sum(acc$DRUNK_DR != 0)
# Load the person CSV file
per <- read.csv("person.csv", stringsAsFactors=FALSE)
# names(per)
# Get the subset of data identifying drivers (1)
per_type <- per %>% filter(PER_TYP == 1)
acc %>% ggplot(aes(x = wday(DAY_WEEK))) + geom_bar()
acc %>% ggplot(aes(x = wday(DAY_WEEK, labe=TRUE))) + geom_bar()
acc %>% ggplot(aes(x = wday(DAY_WEEK, label=TRUE))) + geom_bar()
acc %>% ggplot(aes(x = wday(DAY_WEEK, label=TRUE))) + geom_bar() + labs(x = "Day of Week", y = "Number of Accidents")
# Come back and clean this presentation up
acc %>% ggplot(aes(x = hour(HOUR, labels=TRUE)) + geom_bar() + coord_flip(xlim=c(0, 23))
# Come back and clean this presentation up
acc %>% ggplot(aes(x = hour(HOUR, labels=TRUE)) + geom_bar() + coord_flip(xlim=c(0, 23))
# Come back and clean this presentation up
acc %>% ggplot(aes(x = hour(HOUR, labels=TRUE))) + geom_bar() + coord_flip(xlim=c(0, 23))
# Come back and clean this presentation up
acc %>% ggplot(aes(x = hour(HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23))
# Come back and clean this presentation up
acc %>% ggplot(aes(x = hour(HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23))
acc %>% ggplot(aes(x = hour(HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23))
acc %>% ggplot(aes(x = hour(HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23))
acc %>% ggplot(aes(x = hour(HOUR))) + geom_bar() + coord_flip(xlim=c(0, 23))
acc %>% ggplot(aes(x = hour(HOUR))) + geom_bar() #+ coord_flip(xlim=c(0, 23))
acc$HOUR
hour(23)
hour(23, origin=lubridate::origin)
hour(acc$HOUR)
hour(acc$HOUR, origin = "00:00")
hour(acc$HOUR)
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23))
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23)) + labs(x = "Hour of Day", y = "Number of Accidents")
head(driver)
# Get the subset of data identifying drivers (1)
driver <- per %>% filter(PER_TYP == 1)
head(driver)
names(driver)
head(driver$DAY)
names(driver)
names(acc)
# Join the accidents data , GLCs, and appropriate map data
GLC <- readxl::read_xlsx
names(GLC)
str(GLC)
# Join the accidents data , GLCs, and appropriate map data
GLC <- readxl::read_xlsx("GLC.xlsx")
names(GLC)
readxl::read_xlsx()
?readxl::read_xlsx()
# Join the accidents data , GLCs, and appropriate map data
GLC <- readxl::read_xlsx("GLC.xlsx")
names(GLC)
head(driver$COUNTY)
names(driver)
head(driver$COUNTY)
head(driver$STATE)
rename(GLC, c("State Name" = "STATE", "County Code" = "COUNTY"))
names(GLC)
library(plyr)
?rename
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(dplyr)
library(ggplot2)
rm(list=ls())
rename(GLC, replace = c("State Name" = "STATE", "County Code" = "COUNTY"))
GLC <- rename(GLC, replace = c("State Name" = "STATE", "County Code" = "COUNTY"))
# Join the accidents data , GLCs, and appropriate map data
GLC <- readxl::read_xlsx("GLC.xlsx")
names(GLC)
GLC <- rename(GLC, replace = c("State Name" = "STATE", "County Code" = "COUNTY"))
names(GLC)
?left_join
# Try joining the datasets
dat <- acc %>% left_join(driver, by = c("STATE", "COUNTY"))
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(dplyr)
library(ggplot2)
rm(list=ls())
# Load the accident CSV file
acc <- read.csv("accident.csv", stringsAsFactors=FALSE)
names(acc)
acc %>% ggplot(aes(x = wday(DAY_WEEK, label=TRUE))) + geom_bar() + labs(x = "Day of Week", y = "Number of Accidents")
# I like switching to the horizontal bar chart, but I don't like the lack of labels
acc %>% ggplot(aes(x = HOUR)) + geom_bar() + coord_flip(xlim=c(0, 23)) + labs(x = "Hour of Day", y = "Number of Accidents")
sum(acc$DRUNK_DR != 0)
# Load the person CSV file
per <- read.csv("person.csv", stringsAsFactors=FALSE)
# names(per)
# Get the subset of data identifying drivers (1)
driver <- per %>% filter(PER_TYP == 1)
names(driver)
head(driver$COUNTY)
head(driver$STATE)
# Join the accidents data , GLCs, and appropriate map data
GLC <- readxl::read_xlsx("GLC.xlsx")
# Convert the column names we need for the merge
GLC <- rename(GLC, replace = c("State Name" = "STATE", "County Code" = "COUNTY"))
names(GLC)
# Try joining the datasets
dat <- acc %>% left_join(driver, by = c("STATE", "COUNTY"))
dat <- dat %>% left_join(GLC, by = c("STATE", "COUNTY"))
# names(GLC)
str(GLC)
# Clean up GLC data -
GLC$`State Code` <- as.numeric(GLC$`State Code`)
# names(GLC)
str(GLC)
GLC$COUNTY <- as.numeric(GLC$COUNTY)
# names(GLC)
str(GLC)
str(acc)
# names(driver)
# head(driver$COUNTY)
# head(driver$STATE)
str(driver)
# Join the accidents data , GLCs, and appropriate map data
GLC <- readxl::read_xlsx("GLC.xlsx")
# Convert the column names we need for the merge
GLC <- rename(GLC, replace = c("State Name" = "STATE", "County Code" = "COUNTY"))
# Clean up GLC data -
GLC$`State Code` <- as.numeric(GLC$`State Code`)
GLC$COUNTY <- as.numeric(GLC$COUNTY)
# names(GLC)
str(GLC)
# Try joining the datasets
dat <- acc %>% left_join(driver, by = c("STATE", "COUNTY"))
dat <- dat %>% left_join(GLC, by = c("STATE", "COUNTY"))
str(dat)
# names(GLC)
str(GLC)
# Join the accidents data , GLCs, and appropriate map data
GLC <- readxl::read_xlsx("GLC.xlsx")
# Convert the column names we need for the merge
GLC <- rename(GLC, replace = c("State Code" = "STATE", "County Code" = "COUNTY"))
# Clean up GLC data -
GLC$`State Code` <- as.numeric(GLC$`State Code`)
# Join the accidents data , GLCs, and appropriate map data
GLC <- readxl::read_xlsx("GLC.xlsx")
# Convert the column names we need for the merge
GLC <- rename(GLC, replace = c("State Code" = "STATE", "County Code" = "COUNTY"))
# Clean up GLC data -
GLC$`STATE` <- as.numeric(GLC$`STATE`)
GLC$COUNTY <- as.numeric(GLC$COUNTY)
# names(GLC)
# str(GLC)
# Try joining the datasets
dat <- acc %>% left_join(driver, by = c("STATE", "COUNTY"))
# str(dat)
dat <- dat %>% left_join(GLC, by = c("STATE", "COUNTY"))
